<!doctype html><html lang=en class=no-js><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-36037335-10"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag('js',new Date),gtag('config','UA-36037335-10')</script><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.83.1"><meta name=ROBOTS content="NOINDEX, NOFOLLOW"><link rel=canonical type=text/html href=https://example.com/blog/><link rel="shortcut icon" type=image/png href=/images/favicon.png><link rel=apple-touch-icon href=/favicons/apple-touch-icon-180x180.png sizes=180x180><link rel=manifest href=/manifest.webmanifest><link rel=apple-touch-icon href=/images/kubernetes-192x192.png><title>Team Blog | Kubernetes</title><meta property="og:title" content="Team Blog"><meta property="og:description" content="Production-Grade Container Orchestration"><meta property="og:type" content="website"><meta property="og:url" content="https://example.com/blog/"><meta property="og:site_name" content="Kubernetes"><meta itemprop=name content="Team Blog"><meta itemprop=description content="Production-Grade Container Orchestration"><meta name=twitter:card content="summary"><meta name=twitter:title content="Team Blog"><meta name=twitter:description content="Production-Grade Container Orchestration"><link rel=preload href=/scss/main.min.7977f49df74ac0bb6ddcd5992b1805a268874812a2f606fd43a78d671d946193.css as=style><link href=/scss/main.min.7977f49df74ac0bb6ddcd5992b1805a268874812a2f606fd43a78d671d946193.css rel=stylesheet integrity><script src=/js/jquery-3.3.1.min.js integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8=" crossorigin=anonymous></script><script type=application/ld+json>{"@context":"https://schema.org","@type":"Organization","url":"https://kubernetes.io","logo":"https://kubernetes.io/images/favicon.png","potentialAction":{"@type":"SearchAction","target":"https://example.com/search/?q={search_term_string}","query-input":"required name=search_term_string"}}</script><meta name=theme-color content="#326ce5"><link rel=stylesheet href=/css/feature-states.css><meta name=description content><meta property="og:description" content><meta name=twitter:description content><meta property="og:url" content="https://example.com/blog/"><meta property="og:title" content="Team Blog"><meta name=twitter:title content="Team Blog"><meta name=twitter:image content="https://kubernetes.io/images/favicon.png"><meta name=twitter:image:alt content="Kubernetes"><meta property="og:type" content="article"><script src=/js/script.js></script></head><body class="td-section td-blog"><header><nav class="js-navbar-scroll navbar navbar-expand navbar-dark flex-row td-navbar" data-auto-burger=primary><a class=navbar-brand href=/></a><div class="td-navbar-nav-scroll ml-md-auto" id=main_navbar><ul class="navbar-nav mt-2 mt-lg-0"><li class="nav-item mr-2 mb-lg-0"><a class="nav-link active" href=/blog/>Team Blog</span></a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/community/>Community</span></a></li></ul></div><button id=hamburger onclick=kub.toggleMenu() data-auto-burger-exclude><div></div></button></nav></header><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><div class="col-12 col-md-3 col-xl-2 td-sidebar d-print-none"></div><div class="d-none d-xl-block col-xl-2 td-toc d-print-none"></div><main class="col-12 col-md-9 col-xl-8 pl-md-5 pr-md-4" role=main><div class=td-content><div class="pageinfo pageinfo-primary d-print-none"><p>This the multi-page printable view of this section.
<a href=# onclick="return print(),!1">Click here to print</a>.</p><p><a href=/blog/>Return to the regular view of this page</a>.</p></div><h1 class=title>Team Blog</h1><ul><li><a href=#pg-829a39e6b94078952ee2e58702fdd586>Windows workers with MicroK8s</a></li><li><a href=#pg-43b9235c203c80cdfc0dc9fd276f6321>Graceful Node Shutdown Goes Beta</a></li><li><a href=#pg-69e11eee0766a8c8b934b7a30143dc2b>The Evolution of Kubernetes Dashboard</a></li></ul><div class=content></div></div><div class=td-content><h1 id=pg-829a39e6b94078952ee2e58702fdd586>Windows workers with MicroK8s</h1><div class="td-byline mb-4"><time datetime=2021-04-27 class=text-muted>Tuesday, April 27, 2021</time></div><h1 id=windows-containers-on-kubernetes-with-microk8s>Windows containers on Kubernetes with MicroK8s</h1><p>Kubernetes orchestrates clusters of machines to run container-based workloads. Building on the success
of the container-based development model, it provides the tools to operate containers reliably at
scale. The container-based development methodology is popular outside just the realm of open source
and Linux though. Exactly the same benefits of containers - low resource overhead, dependency management,
faster development cycles, portability and consistent operation - apply to applications targeting the
Windows OS also.</p><h3 id=kubernetes-on-windows>Kubernetes on Windows</h3><p>There are no plans to develop code for a Windows only Kubernetes cluster. The control plane and
master components (kube-apiserver, kube-scheduler, kube-controller) will, for the foreseeable future,
only run on a Linux OS. However, it is possible to run the services required for a Kubernetes node on Windows.</p><p>This means that a worker node can be used to run Windows workloads, while the control plane runs
on Linux - a hybrid cluster. Production-level support for running a Windows node was introduced
with <a href=https://kubernetes.io/blog/2019/03/25/kubernetes-1-14-release-announcement/>version 1.14 of Kubernetes back in March 2019</a>, so in terms of deployment of Windows containers, operators can expect exactly the same features for Windows workloads as they do for Linux ones.</p><p>The Kubernetes components that are required for a worker node are:</p><ul><li><p>kubelet: this is the Kubernetes agent which manages and reports on the running containers in a pod.</p></li><li><p>kube-proxy: a network proxy component which maintains the network rules and allows pods to
communicate within or externally to the rest of the cluster</p></li><li><p>a container runtime: the executable responsible for running individual containers. Actually, several
different container runtimes are now supported by Kubernetes - Docker, CRI-O, containerd (and through
that, many others such as the Windows-specific runhcs), as well as any other which support the
Kubernetes <a href=https://github.com/kubernetes/community/blob/master/contributors/devel/sig-node/container-runtime-interface.md>container runtime interface (CRI)</a></p></li></ul><p><img src=image_0.png alt="image alt text"></p><p>Kubernetes cluster components (image CC BY 4.0 The Kubernetes Authors).</p><p>It is of course possible to manually install these components on a Windows machine and construct a node,
then integrate it into an existing cluster. There are easier ways to achieve this though.</p><h3 id=microk8s-and-calico>MicroK8s and Calico</h3><p>For the Linux-based part of a hybrid Kubernetes cluster, MicroK8s is a compelling choice.
MicroK8s is a minimal implementation of Kubernetes which can run on the average laptop, yet has production
grade features. It's great for offline development, prototyping and testing, and if required you can also
get professional support for it through <a href=https://ubuntu.com/support>Ubuntu Advantage</a>.</p><p>The most compelling feature of MicroK8s for this use case is that you can run it sort-of-almost-natively
on Windows 10! For development work and even production use-cases, this is hugely useful - your hybrid
cluster can reside on one physical Windows machine. MicroK8s on Windows works by making use of
<a href=https://multipass.run/>Multipass</a> - a neat way of running a virtual Ubuntu machine on Windows.
The MicroK8s installer for Windows doesn't require any knowledge of multi-pass or even virtual machines
though, it sets everything up with just a few options from the installer.</p><p>To fetch the current installer and see the (simple!) install instructions for MicroK8s on Windows, check out the <a href=https://microk8s.io/>MicroK8s website</a>.</p><p>Whether using MicroK8 on Windows or a separate Linux machine, the next thing to consider is networking. Calico has been the default CNI(Container Network Interface) for MicroK8s since the 1.19 release. The CNI handles a number of networking requirements:</p><ul><li><p>Container-to-container communications</p></li><li><p>Pod-to-Pod communications</p></li><li><p>Pod-to-Service communications</p></li><li><p>External-to-Service communications</p></li></ul><p>The popularity of Calico has been building for some time over simpler CNIs, as it supports
more useful features (e.g. Border Gateway Protocol [#ref]) and is also available in a
<a href=https://www.tigera.io/tigera-products/calico-enterprise/>supported enterprise version</a>.</p><p>Usefully, Calico already bundles the Windows executables for this CNI in an installer script that also fetches the other components required for a Windows node, making installation and setup that much easier. The Calico website has <a href=https://docs.projectcalico.org/getting-started/windows-calico/quickstart>complete instructions for this</a>. This contains not only links to the installer script but also a detailed rationale on how the setup works. If you are just interested in getting started with adding a Windows worker with MicroK8s there are more concise and specific instructions in the <a href=https://microk8s.io/docs/add-a-windows-worker-node-to-microk8s>MicroK8s documentation</a>.</p><h3 id=scheduling-windows-containers-on-hybrid-kubernetes>Scheduling Windows containers on hybrid Kubernetes</h3><p>The final step of this journey is to actually deploy some Windows containers. With a
hybrid cluster, you could potentially be running Windows or Linux based containers and
Kubernetes will need some way of knowing which nodes to deploy them on.</p><p>A pod specification for a Windows workload should contain a nodeselector field
identifying that it is to run on Windows. The recommended additional workflow is to use
the 'taints and tolerations' features of Kubernetes, to explicitly refuse deployments
on the Windows nodes to any pod which hasn't specifically requested them. There is a
full explanation of this workflow in the upstream
<a href=https://kubernetes.io/docs/setup/production-environment/windows/user-guide-windows-containers/#taints-and-tolerations>Kubernetes documentation</a>.</p><p>More from MicroK8s</p><p>There is plenty more you can do with a MicroK8s cluster - be sure to check out the
documentation for <a href=https://microk8s.io/docs/addons>MicroK8s add-ons</a> so you can
enable the dashboard, ingress controllers, Kubeflow and more.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-43b9235c203c80cdfc0dc9fd276f6321>Graceful Node Shutdown Goes Beta</h1><div class="td-byline mb-4"><time datetime=2021-04-21 class=text-muted>Wednesday, April 21, 2021</time></div><p><strong>Authors:</strong> David Porter (Google), Mrunal Patel (Red Hat), and Tim Bannister (The Scale Factory)</p><p>Graceful node shutdown, beta in 1.21, enables kubelet to gracefully evict pods during a node shutdown.</p><p>Kubernetes is a distributed system and as such we need to be prepared for inevitable failures — nodes will fail, containers might crash or be restarted, and - ideally - your workloads will be able to withstand these catastrophic events.</p><p>One of the common classes of issues are workload failures on node shutdown or restart. The best practice prior to bringing your node down is to <a href=/docs/tasks/administer-cluster/safely-drain-node/>safely drain and cordon your node</a>. This will ensure that all pods running on this node can safely be evicted. An eviction will ensure your pods can follow the expected <a href=/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination>pod termination lifecycle</a> meaning receiving a SIGTERM in your container and/or running <code>preStopHooks</code>.</p><p>Prior to Kubernetes 1.20 (when graceful node shutdown was introduced as an alpha feature), safe node draining was not easy: it required users to manually take action and drain the node beforehand. If someone or something shut down your node without draining it first, most likely your pods would not be safely evicted from your node and shutdown abruptly. Other services talking to those pods might see errors due to the pods exiting abruptly. Some examples of this situation may be caused by a reboot due to security patches or preemption of short lived cloud compute instances.</p><p>Kubernetes 1.21 brings graceful node shutdown to beta. Graceful node shutdown gives you more control over some of those unexpected shutdown situations. With graceful node shutdown, the kubelet is aware of underlying system shutdown events and can propagate these events to pods, ensuring containers can shut down as gracefully as possible. This gives the containers a chance to checkpoint their state or release back any resources they are holding.</p><p>Note, that for the best availability, even with graceful node shutdown, you should still design your deployments to be resilient to node failures.</p><h2 id=how-does-it-work>How does it work?</h2><p>On Linux, your system can shut down in many different situations. For example:</p><ul><li>A user or script running <code>shutdown -h now</code> or <code>systemctl poweroff</code> or <code>systemctl reboot</code>.</li><li>Physically pressing a power button on the machine.</li><li>Stopping a VM instance on a cloud provider, e.g. <code>gcloud compute instances stop</code> on GCP.</li><li>A Preemptible VM or Spot Instance that your cloud provider can terminate unexpectedly, but with a brief warning.</li></ul><p>Many of these situations can be unexpected and there is no guarantee that a cluster administrator drained the node prior to these events. With the graceful node shutdown feature, kubelet uses a systemd mechanism called <a href=https://www.freedesktop.org/wiki/Software/systemd/inhibit>"Inhibitor Locks"</a> to allow draining in most cases. Using Inhibitor Locks, kubelet instructs systemd to postpone system shutdown for a specified duration, giving a chance for the node to drain and evict pods on the system.</p><p>Kubelet makes use of this mechanism to ensure your pods will be terminated cleanly. When the kubelet starts, it acquires a systemd delay-type inhibitor lock. When the system is about to shut down, the kubelet can delay that shutdown for a configurable, short duration utilizing the delay-type inhibitor lock it acquired earlier. This gives your pods extra time to terminate. As a result, even during unexpected shutdowns, your application will receive a SIGTERM, <a href=/docs/concepts/containers/container-lifecycle-hooks/#container-hooks>preStop hooks</a> will execute, and kubelet will properly update <code>Ready</code> node condition and respective pod statuses to the api-server.</p><p>For example, on a node with graceful node shutdown enabled, you can see that the inhibitor lock is taken by the kubelet:</p><pre><code>kubelet-node ~ # systemd-inhibit --list
    Who: kubelet (UID 0/root, PID 1515/kubelet)
    What: shutdown
    Why: Kubelet needs time to handle node shutdown
    Mode: delay

1 inhibitors listed.
</code></pre><p>One important consideration we took when designing this feature is that not all pods are created equal. For example, some of the pods running on a node such as a logging related daemonset should stay running as long as possible to capture important logs during the shutdown itself. As a result, pods are split into two categories: "regular" and "critical". <a href=/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/#marking-pod-as-critical>Critical pods</a> are those that have <code>priorityClassName</code> set to <code>system-cluster-critical</code> or <code>system-node-critical</code>; all other pods are considered regular.</p><p>In our example, the logging DaemonSet would run as a critical pod. During the graceful node shutdown, regular pods are terminated first, followed by critical pods. As an example, this would allow a critical pod associated with a logging daemonset to continue functioning, and collecting logs during the termination of regular pods.</p><p>We will evaluate during the beta phase if we need more flexibility for different pod priority classes and add support if needed, please let us know if you have some scenarios in mind.</p><h2 id=how-do-i-use-it>How do I use it?</h2><p>Graceful node shutdown is controlled with the <code>GracefulNodeShutdown</code> <a href=/docs/reference/command-line-tools-reference/feature-gates>feature gate</a> and is enabled by default in Kubernetes 1.21.</p><p>You can configure the graceful node shutdown behavior using two kubelet configuration options: <code>ShutdownGracePeriod</code> and <code>ShutdownGracePeriodCriticalPods</code>. To configure these options, you edit the kubelet configuration file that is passed to kubelet via the <code>--config</code> flag; for more details, refer to <a href=/docs/tasks/administer-cluster/kubelet-config-file/>Set kubelet parameters via a configuration file</a>.</p><p>During a shutdown, kubelet terminates pods in two phases. You can configure how long each of these phases lasts.</p><ol><li>Terminate regular pods running on the node.</li><li>Terminate critical pods running on the node.</li></ol><p>The settings that control the duration of shutdown are:</p><ul><li><code>ShutdownGracePeriod</code><ul><li>Specifies the total duration that the node should delay the shutdown by. This is the total grace period for pod termination for both regular and critical pods.</li></ul></li><li><code>ShutdownGracePeriodCriticalPods</code><ul><li>Specifies the duration used to terminate critical pods during a node shutdown. This should be less than <code>ShutdownGracePeriod</code>.</li></ul></li></ul><p>For example, if <code>ShutdownGracePeriod=30s</code>, and <code>ShutdownGracePeriodCriticalPods=10s</code>, kubelet will delay the node shutdown by 30 seconds. During this time, the first 20 seconds (30-10) would be reserved for gracefully terminating normal pods, and the last 10 seconds would be reserved for terminating critical pods.</p><p>Note that by default, both configuration options described above, <code>ShutdownGracePeriod</code> and <code>ShutdownGracePeriodCriticalPods</code> are set to zero, so you will need to configure them as appropriate for your environment to activate graceful node shutdown functionality.</p><h2 id=how-can-i-learn-more>How can I learn more?</h2><ul><li>Read the <a href=/docs/concepts/architecture/nodes/#graceful-node-shutdown>documentation</a></li><li>Read the enhancement proposal, <a href=https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/2000-graceful-node-shutdown>KEP 2000</a></li><li>View the <a href=https://github.com/kubernetes/kubernetes/tree/release-1.21/pkg/kubelet/nodeshutdown>code</a></li></ul><h2 id=how-do-i-get-involved>How do I get involved?</h2><p>Your feedback is always welcome! SIG Node meets regularly and can be reached via <a href=https://slack.k8s.io>Slack</a> (channel <code>#sig-node</code>), or the SIG's <a href=https://github.com/kubernetes/community/tree/master/sig-node#contact>mailing list</a></p></div><div class=td-content style=page-break-before:always><h1 id=pg-69e11eee0766a8c8b934b7a30143dc2b>The Evolution of Kubernetes Dashboard</h1><div class="td-byline mb-4"><time datetime=2021-03-09 class=text-muted>Tuesday, March 09, 2021</time></div><p>Authors: Marcin Maciaszczyk, Kubermatic & Sebastian Florek, Kubermatic</p><p>In October 2020, the Kubernetes Dashboard officially turned five. As main project maintainers, we can barely believe that so much time has passed since our very first commits to the project. However, looking back with a bit of nostalgia, we realize that quite a lot has happened since then. Now it’s due time to celebrate “our baby” with a short recap.</p><h2 id=how-it-all-began>How It All Began</h2><p>The initial idea behind the Kubernetes Dashboard project was to provide a web interface for Kubernetes. We wanted to reflect the kubectl functionality through an intuitive web UI. The main benefit from using the UI is to be able to quickly see things that do not work as expected (monitoring and troubleshooting). Also, the Kubernetes Dashboard is a great starting point for users that are new to the Kubernetes ecosystem.</p><p>The very <a href=https://github.com/kubernetes/dashboard/commit/5861187fa807ac1cc2d9b2ac786afeced065076c>first commit</a> to the Kubernetes Dashboard was made by Filip Grządkowski from Google on 16th October 2015 – just a few months from the initial commit to the Kubernetes repository. Our initial commits go back to November 2015 (<a href=https://github.com/kubernetes/dashboard/commit/09e65b6bb08c49b926253de3621a73da05e400fd>Sebastian committed on 16 November 2015</a>; <a href=https://github.com/kubernetes/dashboard/commit/1da4b1c25ef040818072c734f71333f9b4733f55>Marcin committed on 23 November 2015</a>). Since that time, we’ve become regular contributors to the project. For the next two years, we worked closely with the Googlers, eventually becoming main project maintainers ourselves.</p><figure><img src=/blog/2021/03/09/the-evolution-of-kubernetes-dashboard/first-ui.png alt="The First Version of the User Interface"><figcaption><p>The First Version of the User Interface</p></figcaption></figure><figure><img src=/blog/2021/03/09/the-evolution-of-kubernetes-dashboard/along-the-way-ui.png alt="Prototype of the New User Interface"><figcaption><p>Prototype of the New User Interface</p></figcaption></figure><figure><img src=/blog/2021/03/09/the-evolution-of-kubernetes-dashboard/current-ui.png alt="The Current User Interface"><figcaption><p>The Current User Interface</p></figcaption></figure><p>As you can see, the initial look and feel of the project were completely different from the current one. We have changed the design multiple times. The same has happened with the code itself.</p><h2 id=growing-up-the-big-migration>Growing Up - The Big Migration</h2><p>At <a href=https://github.com/kubernetes/dashboard/pull/2727>the beginning of 2018</a>, we reached a point where AngularJS was getting closer to the end of its life, while the new Angular versions were published quite often. A lot of the libraries and the modules that we were using were following the trend. That forced us to spend a lot of the time rewriting the frontend part of the project to make it work with newer technologies.</p><p>The migration came with many benefits like being able to refactor a lot of the code, introduce design patterns, reduce code complexity, and benefit from the new modules. However, you can imagine that the scale of the migration was huge. Luckily, there were a number of contributions from the community helping us with the resource support, new Kubernetes version support, i18n, and much more. After many long days and nights, we finally released the <a href=https://github.com/kubernetes/dashboard/releases/tag/v2.0.0-beta1>first beta version</a> in July 2019, followed by the <a href=https://github.com/kubernetes/dashboard/releases/tag/v2.0.0>2.0 release</a> in April 2020 — our baby had grown up.</p><h2 id=where-are-we-standing-in-2021>Where Are We Standing in 2021?</h2><p>Due to limited resources, unfortunately, we were not able to offer extensive support for many different Kubernetes versions. So, we’ve decided to always try and support the latest Kubernetes version available at the time of the Kubernetes Dashboard release. The latest release, <a href=https://github.com/kubernetes/dashboard/releases/tag/v2.2.0>Dashboard v2.2.0</a> provides support for Kubernetes v1.20.</p><p>On top of that, we put in a great deal of effort into <a href=https://github.com/kubernetes/dashboard/issues/5232>improving resource support</a>. Meanwhile, we do offer support for most of the Kubernetes resources. Also, the Kubernetes Dashboard supports multiple languages: English, German, French, Japanese, Korean, Chinese (Traditional, Simplified, Traditional Hong Kong). Persian and Russian localizations are currently in progress. Moreover, we are working on the support for 3rd party themes and the design of the app in general. As you can see, quite a lot of things are going on.</p><p>Luckily, we do have regular contributors with domain knowledge who are taking care of the project, updating the Helm charts, translations, Go modules, and more. But as always, there could be many more hands on deck. So if you are thinking about contributing to Kubernetes, keep us in mind ;)</p><h2 id=what-s-next>What’s Next</h2><p>The Kubernetes Dashboard has been growing and prospering for more than 5 years now. It provides the community with an intuitive Web UI, thereby decreasing the complexity of Kubernetes and increasing its accessibility to new community members. We are proud of what the project has achieved so far, but this is by far not the end. These are our priorities for the future:</p><ul><li>Keep providing support for the new Kubernetes versions</li><li>Keep improving the support for the existing resources</li><li>Keep working on auth system improvements</li><li><a href=https://github.com/kubernetes/dashboard/pull/5449>Rewrite the API to use gRPC and shared informers</a>: This will allow us to improve the performance of the application but, most importantly, to support live updates coming from the Kubernetes project. It is one of the most requested features from the community.</li><li>Split the application into two containers, one with the UI and the second with the API running inside.</li></ul><h2 id=the-kubernetes-dashboard-in-numbers>The Kubernetes Dashboard in Numbers</h2><ul><li>Initial commit made on October 16, 2015</li><li>Over 100 million pulls from Dockerhub since the v2 release</li><li>8 supported languages and the next 2 in progress</li><li>Over 3360 closed PRs</li><li>Over 2260 closed issues</li><li>100% coverage of the supported core Kubernetes resources</li><li>Over 9000 stars on GitHub</li><li>Over 237 000 lines of code</li></ul><h2 id=join-us>Join Us</h2><p>As mentioned earlier, we are currently looking for more people to help us further develop and grow the project. We are open to contributions in multiple areas, i.e., <a href="https://github.com/kubernetes/dashboard/issues?q=is%3Aissue+is%3Aopen+label%3A%22help+wanted%22">issues with help wanted label</a>. Please feel free to reach out via GitHub or the #sig-ui channel in the <a href=https://slack.k8s.io/>Kubernetes Slack</a>.</p></div></main></div></div><footer class=d-print-none><div class=footer__links><nav><a class=text-white href=/blog/>Blog</a>
<a class=text-white href=/community/>Community</a></nav></div><div class=container-fluid><div class=row><div class="col-6 col-sm-2 text-xs-center order-sm-2"><ul class="list-inline mb-0"><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Twitter aria-label=Twitter><a class=text-white target=_blank href=https://twitter.com/ubuntu><i class="fab fa-twitter"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Youtube aria-label=Youtube><a class=text-white target=_blank href=https://youtube.com/kubernetescommunity><i class="fab fa-youtube"></i></a></li></ul></div><div class="col-6 col-sm-2 text-right text-xs-center order-sm-3"><ul class="list-inline mb-0"><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=GitHub aria-label=GitHub><a class=text-white target=_blank href=https://github.com/kubernetes/kubernetes><i class="fab fa-github"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Slack aria-label=Slack><a class=text-white target=_blank href=https://slack.k8s.io><i class="fab fa-slack"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Contribute aria-label=Contribute><a class=text-white target=_blank href=https://git.k8s.io/community/contributors/guide><i class="fas fa-edit"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="Stack Overflow" aria-label="Stack Overflow"><a class=text-white target=_blank href=https://stackoverflow.com/questions/tagged/kubernetes><i class="fab fa-stack-overflow"></i></a></li></ul></div><div class="col-12 col-sm-8 text-center order-sm-2"><br><small class=text-white>Copyright &copy; 2021 The Linux Foundation &reg;. All rights reserved. The Linux Foundation has registered trademarks and uses trademarks. For a list of trademarks of The Linux Foundation, please see our <a href=https://www.linuxfoundation.org/trademark-usage class=light-text>Trademark Usage page</a></small><br><small class=text-white>ICP license: 京ICP备17074266号-3</small></div></div></div></footer></div><script src=/js/popper-1.14.3.min.js integrity=sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49 crossorigin=anonymous></script><script src=/js/bootstrap-4.3.1.min.js integrity=sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM crossorigin=anonymous></script><script src=/js/main.min.631e695cd1b79b859f7c95801f5bb473ea0d4e829e3eb2a573281e7f0d67bf4c.js integrity="sha256-Yx5pXNG3m4WffJWAH1u0c+oNToKePrKlcygefw1nv0w=" crossorigin=anonymous></script></body></html>